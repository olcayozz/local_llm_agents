version: "3.8"

services:

#ollama is a local LLM serving platform
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data2:/root/.ollama
    entrypoint: ["ollama", "run", "hf.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF:Q8_0"]
    restart: unless-stopped
    networks:
      - front-tier

# open-webui is a web UI for Ollama
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - front-tier

# local API demonstration platform  
  local_API:
    build:
      context: ./services/local_api
      dockerfile: local_api.Dockerfile
    image: llm/local_api:latest
    hostname: local_api
    domainname: local_api.com
    container_name: local_api
    ports:
      - "5000:5000"
    volumes:
      - local_api:/root/app
    networks:
      - back-tier
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

# python dev is for development purposes only      
  python_dev:
    build:
      context: ./services/python_dev
      dockerfile: python_dev.Dockerfile
    image: llm/python_dev:latest
    hostname: python_dev
    domainname: python_dev.com
    container_name: python_dev
    volumes:
      - python_dev2:/root/app
      - ./services/python_dev/app:/root/app
    networks:
      - front-tier
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
    
  agents:
    build:
      context: ./services/agents
      dockerfile: agents.Dockerfile
    image: llm/agents:latest
    hostname: agents
    domainname: agents.com
    container_name: agents
    volumes:
      - agents2:/root/app
      - ./services/agents/app:/root/app
    networks:
      - front-tier
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped

# simple_agent is agentic app demonstration platform
  simple_agent:
    build:
      context: ./services/simple_agent
      dockerfile: simple_agent.Dockerfile
    image: llm/simple_agent:latest
    hostname: simple_agent
    domainname: simple_agent.com
    container_name: simple_agent
    volumes:
      #- simple_agent:/root/app
      - ./services/simple_agent/app:/root/app
    networks:
      - front-tier
      - back-tier
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - OLLAMA_MODEL=hf.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF:Q8_0
    depends_on:
      - ollama
      - qdrant
    restart: unless-stopped

# vector database for agentic app
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"   # REST API
      - "6334:6334"   # gRPC API
    volumes:
      - qdrant_data2:/qdrant/storage
    restart: unless-stopped
    networks:
      - back-tier

volumes:
  local_api:
  ollama_data:
  python_dev:
  simple_agent:
  agents:
  qdrant_data:


networks:
  front-tier:
  back-tier: