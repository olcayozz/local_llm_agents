version: "3.8"

services:

#ollama is a local LLM serving platform
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: ["/bin/sh", "-c", "ollama serve && sleep 5 && ollama run hf.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF:Q8_0"]
    restart: unless-stopped
    networks:
      - front-tier
    deploy:
      resources:
        limits:
          cpus: '8.0'
          memory: 12G
        reservations:
          cpus: '4.0'
          memory: 8G

# open-webui is a web UI for Ollama
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - front-tier
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

# local API demonstration platform
  local_API:
    build:
      context: ./services/local_api
      dockerfile: local_api.Dockerfile
    image: llm/local_api:latest
    hostname: local_api
    domainname: local_api.com
    container_name: local_api
    ports:
      - "5000:5000"
    volumes:
      - local_api:/root/app
    networks:
      - back-tier
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

# python dev is for development purposes only
  python_dev:
    build:
      context: ./services/python_dev
      dockerfile: python_dev.Dockerfile
    image: llm/python_dev:latest
    hostname: python_dev
    domainname: python_dev.com
    container_name: python_dev
    volumes:
      - python_dev:/root/app
      - ./services/python_dev/app:/root/app
    networks:
      - front-tier
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

  agents:
    build:
      context: ./services/agents
      dockerfile: agents.Dockerfile
    image: llm/agents:latest
    hostname: agents
    domainname: agents.com
    container_name: agents
    volumes:
      - agents:/root/app
      - ./services/agents/app:/root/app
    networks:
      - front-tier
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

# simple_agent is agentic app demonstration platform
  simple_agent:
    build:
      context: ./services/simple_agent
      dockerfile: simple_agent.Dockerfile
    image: llm/simple_agent:latest
    hostname: simple_agent
    domainname: simple_agent.com
    container_name: simple_agent
    volumes:
      #- simple_agent:/root/app
      - ./services/simple_agent/app:/root/app
    networks:
      - front-tier
      - back-tier
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - OLLAMA_MODEL=hf.co/HuggingFaceTB/SmolLM2-360M-Instruct-GGUF:Q8_0
    depends_on:
      - ollama
      - qdrant
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

# Multi-Agent Orchestrator - Complete system with all 29 agents and meeting system
  multi_agent_orchestrator:
    build:
      context: .
      dockerfile: services/multi_agent_orchestrator/orchestrator.Dockerfile
    image: llm/multi_agent_orchestrator:latest
    hostname: multi_agent_orchestrator
    domainname: orchestrator.local
    container_name: multi_agent_orchestrator
    ports:
      - "8000:8000"  # For future API endpoint
    volumes:
      - orchestrator_data:/app/agent_memories
      - orchestrator_logs:/app/logs
      - ./agents:/app/agents
      - ./Role:/app/Role
      - ./examples:/app/examples
    networks:
      - front-tier
      - back-tier
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - MODEL_NAME=llama3.2
      - TEMPERATURE=0.7
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app:/app/agents
    depends_on:
      - ollama
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '8.0'
          memory: 8G
        reservations:
          cpus: '4.0'
          memory: 6G
    healthcheck:
      test: ["CMD", "python", "-c", "from agents.orchestrator import AgentOrchestrator; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

# vector database for agentic app
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"   # REST API
      - "6334:6334"   # gRPC API
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped
    networks:
      - back-tier
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '2.0'
          memory: 2G

volumes:
  local_api:
  ollama_data:
  python_dev:
  simple_agent:
  agents:
  qdrant_data:
  orchestrator_data:
  orchestrator_logs:


networks:
  front-tier:
  back-tier: